{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yRB6H9pNaexV"
      },
      "outputs": [],
      "source": [
        "import sys, os, re, csv, random\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import numpy as np\n",
        "import statistics\n",
        "#import xgboost as xgb\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OSjhA41ka-Rc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from scipy.stats import spearmanr\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YGCrcx07gNo2"
      },
      "outputs": [],
      "source": [
        "def compute_correlation(original_data, label_list, fake_data):\n",
        "\n",
        "  corr_list = []\n",
        "\n",
        "  majority_original = []\n",
        "\n",
        "  for i in range(len(label_list)):\n",
        "    if int(label_list[i])==1:\n",
        "      majority_original.append(original_data[i])\n",
        "  \n",
        "  majority_original = np.array(majority_original)\n",
        "  mean_majority_vec = np.mean(majority_original, axis=0)\n",
        "\n",
        "  sum_corr = 0\n",
        "  for i in range(fake_data.shape[0]):\n",
        "    corr, _ = spearmanr(mean_majority_vec, fake_data[i])\n",
        "    sum_corr += corr\n",
        "  return sum_corr/float(fake_data.shape[0])\n",
        "\n",
        "def compare_datasets(original_dataset, fake_dataset):\n",
        "  \n",
        "  column_list = []\n",
        "  original_index = {1}\n",
        "  fake_index = {original_dataset.shape[0]+1}\n",
        "\n",
        "  for i in range(1, original_dataset.shape[0]):\n",
        "    original_index.add(i+1)\n",
        "  \n",
        "  for i in range(1, fake_dataset.shape[0]):\n",
        "    fake_index.add(original_dataset.shape[0]+i+1)\n",
        "  \n",
        "  for i in range(41):\n",
        "    column_list.append(\"OC_\"+ str(i+1))\n",
        "\n",
        "  original_df = pd.DataFrame(original_dataset, columns=column_list, index=original_index)\n",
        "  original_df.drop_duplicates()\n",
        "  fake_df = pd.DataFrame(fake_dataset, columns=column_list, index=fake_index)\n",
        "  fake_df.drop_duplicates()\n",
        "\n",
        "\n",
        "  #original_df.reset_index(drop=True)\n",
        "  #fake_df.reset_index(drop=True)\n",
        "\n",
        "  #print(original_df.shape)\n",
        "  #print(fake_df.shape)\n",
        "\n",
        "\n",
        "  table_evaluator = TableEvaluator(original_df, fake_df)\n",
        "  table_evaluator.visual_evaluation()\n",
        "\n",
        "def apply_gaussian_noise(data_list, label_list):\n",
        "  original_dataset = np.copy(data_list)\n",
        "\n",
        "  index_list = []\n",
        "  for i in range(len(label_list)):\n",
        "    if int(label_list[i])==1:\n",
        "      index_list.append(i)\n",
        "  \n",
        "  for i in range(len(index_list)-1):\n",
        "    for j in range(i+1, len(index_list)):\n",
        "      \n",
        "      rand_el = random.choice([i, j])\n",
        "      max_num = np.max(data_list[index_list[rand_el]])\n",
        "      min_num = np.min(data_list[index_list[rand_el]])\n",
        "      if abs(min_num) >= max_num:\n",
        "        gauss = np.random.normal(0,(float(max_num)/float(2)),data_list[index_list[rand_el]].shape)\n",
        "      else:\n",
        "        gauss = np.random.normal(0,(float(abs(min_num))/float(2)),data_list[index_list[rand_el]].shape)\n",
        "      #print(gauss)\n",
        "\n",
        "      new_data_row = data_list[index_list[rand_el]] + gauss\n",
        "\n",
        "      new_data_row = np.reshape(new_data_row, (1, new_data_row.shape[0]))\n",
        "\n",
        "      if i==0 and j==1:\n",
        "        fake_dataset = np.copy(new_data_row)\n",
        "      else:\n",
        "        fake_dataset = np.append(fake_dataset, new_data_row , axis=0)  \n",
        "        \n",
        "\n",
        "      data_list = np.append(data_list, new_data_row , axis=0)\n",
        "      label_list = np.append(label_list, 1)        \n",
        "    \n",
        "    return data_list, label_list, fake_dataset\n",
        "\n",
        "\n",
        "def apply_majority_oversampling(data_list, label_list):\n",
        "\n",
        "    original_dataset = np.copy(data_list)\n",
        "\n",
        "    index_list = []\n",
        "    for i in range(len(label_list)):\n",
        "      if int(label_list[i])==1:\n",
        "        index_list.append(i)\n",
        "    \n",
        "    for i in range(len(index_list)-1):\n",
        "      for j in range(i+1, len(index_list)):\n",
        "        ratio = random.random()\n",
        "        while ratio==0 or ratio==1:\n",
        "          ratio = random.random()\n",
        "\n",
        "        new_data_row = ratio*data_list[index_list[i]] + (1-ratio)*data_list[index_list[j]]\n",
        "\n",
        "        new_data_row = np.reshape(new_data_row, (1, new_data_row.shape[0]))\n",
        "\n",
        "        if i==0 and j==1:\n",
        "          fake_dataset = np.copy(new_data_row)\n",
        "        else:\n",
        "          fake_dataset = np.append(fake_dataset, new_data_row , axis=0)  \n",
        "        \n",
        "\n",
        "        data_list = np.append(data_list, new_data_row , axis=0)\n",
        "        label_list = np.append(label_list, 1)        \n",
        "\n",
        "\n",
        "\n",
        "    #compare_datasets(original_dataset, fake_dataset)\n",
        "    \n",
        "    return data_list, label_list, fake_dataset\n",
        "\n",
        "\n",
        "\n",
        "def apply_smote(data_list, label_list):\n",
        "\n",
        "    #print(\"Before Count: \", Counter(label_list))\n",
        "    \n",
        "    # Apply majority oversampling or gaussina noise\n",
        "    original_data_list = np.copy(data_list)\n",
        "    original_label_list = copy.deepcopy(label_list)\n",
        "\n",
        "    #data_list, label_list, fake_list = apply_majority_oversampling(data_list, label_list)\n",
        "    data_list, label_list, fake_list = apply_gaussian_noise(data_list, label_list)\n",
        "    \n",
        "    #print(data_list.shape)\n",
        "    # Apply SMOTE\n",
        "    transformed_data_list = np.copy(data_list)\n",
        "    #print(\"After Majority Oversampling Count: \", Counter(label_list))\n",
        "    orig_shape = transformed_data_list.shape\n",
        "    transformed_label_list = []\n",
        "\n",
        "    \n",
        "    for i in range(0,transformed_data_list.shape[0]):\n",
        "          transformed_label_list.append(int(label_list[i]))    \n",
        "\n",
        "    #print(\"Original Shape: \", orig_shape)\n",
        "    \n",
        "\n",
        "\n",
        "    oversample = SMOTE(k_neighbors=1)\n",
        "    transformed_data_list, transformed_label_list = oversample.fit_resample(transformed_data_list, transformed_label_list)\n",
        "    #print(len(transformed_label_list))\n",
        "    #print(len(transformed_data_list))\n",
        "    #print(transformed_label_list)\n",
        "    #print(label_list)\n",
        "    added_num = int(transformed_data_list.shape[0]) - int(data_list.shape[0]) \n",
        "\n",
        "\n",
        "    label_list = np.append(label_list, np.zeros(added_num))\n",
        "    #print(\"Updated Label List: \", label_list)\n",
        "\n",
        "    corr_num = compute_correlation(original_data_list, original_label_list, fake_list)\n",
        "\n",
        "    #print(\"After Count: \", Counter(label_list))\n",
        "\n",
        "    return transformed_data_list, label_list, corr_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTWdQf9Ma_O8",
        "outputId": "e01dc510-dc80-453b-d3a4-a951c58f8f52"
      },
      "outputs": [],
      "source": [
        "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "classifier_type = 'svm'\n",
        "\n",
        "\n",
        "data_list = np.load('data_file.npy')\n",
        "label_list = np.load('label_file.npy')\n",
        "\n",
        "\n",
        "print(data_list.shape)\n",
        "\n",
        "round_acc_arr = []\n",
        "round_rec_arr = []\n",
        "round_pre_arr = []\n",
        "round_f1_arr = []\n",
        "round_f1_micro_arr = []\n",
        "round_corr_arr_train = []\n",
        "round_corr_arr_test = []\n",
        "\n",
        "\n",
        "round_acc_cum = 0\n",
        "round_rec_cum = 0\n",
        "round_pre_cum = 0\n",
        "round_f1_cum = 0\n",
        "round_f1_micro_cum = 0\n",
        "\n",
        "round_range = 1000\n",
        "\n",
        "#-------------------------------------------\n",
        "\n",
        "\n",
        "for round_num in range(round_range):\n",
        "\n",
        "  acc_cum = 0\n",
        "  rec_cum = 0\n",
        "  pre_cum = 0\n",
        "  f1_cum = 0\n",
        "  f1_micro_cum = 0\n",
        "  acc_arr = []\n",
        "  rec_arr = []\n",
        "  pre_arr = []\n",
        "  f1_arr = []\n",
        "  f1_micro_arr = []\n",
        "  predicted_label_arr = []\n",
        "  test_label_arr = []\n",
        "  error_analysis = []\n",
        "  fold_number = 1\n",
        "\n",
        "  for train_index, test_index in skf.split(data_list, label_list):\n",
        "      X_train, X_test = data_list[train_index], data_list[test_index]\n",
        "      y_train, y_test = label_list[train_index], label_list[test_index]\n",
        "\n",
        "      X_train, y_train, corr_train = apply_smote(X_train, y_train)\n",
        "      X_test, y_test, corr_test = apply_smote(X_test, y_test)\n",
        "\n",
        "      features_list = ['achievement', 'achievement/effort', 'adaptability/flexibility', 'assisting and caring for others', 'coaching and developing others', 'concern for others',\n",
        "                      'consequence of error', 'conventional', 'cooperation', 'developing and building teams', 'enterprising', 'establishing and maintaining interpersonal relationships',\n",
        "                      'face-to-face discussions', 'freedom to make decisions', 'frequency of conflict situations', 'frequency of decision making',\n",
        "                      'guiding, directing, and motivating subordinates', 'importance of being exact or accurate', 'independence', 'independence_2', 'initiative', 'instructing', 'integrity', 'leadership',\n",
        "                      'level of competition', 'monitoring and controlling resources', 'recognition', 'relationships', 'resolving conflicts and negotiating with others',\n",
        "                      'responsibility for outcomes and results', 'self control', 'service orientation', 'social', 'social orientation', 'stress tolerance',\n",
        "                      'structured versus unstructured work', 'support', 'training and teaching others', 'work schedules', 'work with work group or team', 'working conditions']\n",
        "                      \n",
        "      #print(len(features_list))\n",
        "\n",
        "      if classifier_type=='xgboost':\n",
        "          total_X = np.append(X_train, X_test, axis=0)\n",
        "          total_y = np.append(y_train, y_test, axis=0)\n",
        "          clf = xgb.XGBClassifier(objective=\"binary:logistic\", colsample_bytree=0.5, gamma=0.25, learning_rate=0.1, max_depth=5, reg_lambda=1, scale_pos_weight=5, subsample=0.8, random_state=42)\n",
        "      elif classifier_type=='lr':\n",
        "        clf = LogisticRegression(random_state=42, penalty='l2',solver='liblinear', max_iter=100, C=0.5)\n",
        "      \n",
        "      else:\n",
        "        clf = svm.SVC(random_state=42, C=0.5, kernel='linear')\n",
        "      \n",
        "\n",
        "\n",
        "      clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "      predict_list = []\n",
        "      original_list = []\n",
        "      for i in range(X_test.shape[0]):\n",
        "          predicted_label = clf.predict([X_test[i][:]])\n",
        "          #print(predicted_label)\n",
        "          predict_list.append(str(predicted_label[0]))\n",
        "          original_list.append(str(y_test[i]))\n",
        "      \n",
        "      acc_arr.append(accuracy_score(original_list, predict_list))\n",
        "      acc_cum += acc_arr[fold_number-1]\n",
        "      rec_arr.append(recall_score(original_list, predict_list, average='macro'))\n",
        "      rec_cum += rec_arr[fold_number-1]\n",
        "      pre_arr.append(precision_score(original_list, predict_list, average='macro'))\n",
        "      pre_cum += pre_arr[fold_number-1]\n",
        "      f1_arr.append(f1_score(original_list, predict_list, average='macro'))\n",
        "      f1_cum  += f1_arr[fold_number-1]\n",
        "      f1_micro_arr.append(f1_score(original_list, predict_list, average='micro'))\n",
        "      f1_micro_cum  += f1_micro_arr[fold_number-1]\n",
        "\n",
        "      fold_number += 1\n",
        "\n",
        "\n",
        "  round_acc_cum += acc_cum/5\n",
        "  round_acc_arr.append(acc_cum/5)\n",
        "\n",
        "  round_rec_cum += rec_cum/5\n",
        "  round_rec_arr.append(rec_cum/5)\n",
        "  \n",
        "  round_pre_cum += pre_cum/5\n",
        "  round_pre_arr.append(pre_cum/5)\n",
        "  \n",
        "  round_f1_cum += f1_cum/5\n",
        "  round_f1_arr.append(f1_cum/5)\n",
        "  \n",
        "  round_f1_micro_cum += f1_micro_cum/5\n",
        "  round_f1_micro_arr.append(f1_micro_cum/5)\n",
        "\n",
        "  #print(corr_train)\n",
        "  round_corr_arr_train.append(corr_train)\n",
        "  round_corr_arr_test.append(corr_test)\n",
        "\n",
        "\n",
        "\n",
        "round_corr_arr_train = np.array(round_corr_arr_train)\n",
        "round_corr_arr_test = np.array(round_corr_arr_test)\n",
        "\n",
        "print(\"Train Correlation: \", np.mean(round_corr_arr_train,axis=0))\n",
        "print(\"Test Correlation: \", np.mean(round_corr_arr_test,axis=0))\n",
        "\n",
        "print(\"Accuracy: \", round_acc_cum/round_range)\n",
        "print(\"Recall: \", round_rec_cum/round_range)\n",
        "print(\"Precision: \", round_pre_cum/round_range)\n",
        "print(\"F1 score(macro): \", round_f1_cum/round_range)\n",
        "print(\"F1 score(micro): \", round_f1_micro_cum/round_range)\n",
        "\n",
        "print(\"Accuracy_stdev: \", statistics.stdev(round_acc_arr))\n",
        "print(\"Recall_stdev: \", statistics.stdev(round_rec_arr))\n",
        "print(\"Precision_stdev: \", statistics.stdev(round_pre_arr))\n",
        "print(\"F1(macro) score_stdev: \", statistics.stdev(round_f1_arr))\n",
        "print(\"F1(micro) score_stdev: \", statistics.stdev(round_f1_micro_arr))\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQBB-CB5hzK1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
